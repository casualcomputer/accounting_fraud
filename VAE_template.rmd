---
title: "VAE_template"
author: "casualcomputer"
date: "07/02/2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown


```{r packages,warning=FALSE}
    library(dplyr)
    library(stringr)
    library(scales)
    library(zeallot)
```

## Load data

```{r load_data, echo=TRUE}
# load accounting fraud data     
    dataset <- read.csv("uscecchini28.csv")
    data_dictionary <- read.csv("get_Data_dict.csv") #data dictionary: http://www.crsp.org/products/documentation/annual-data-industrial
    str(dataset)
    str(data_dictionary)
```


## Rename some dataset's columns 
```{r rename_colnames, echo=TRUE}
#  change column names of "dataset", to reflect names of accounting variables
        colNames_dataset <- toupper(names(dataset))
        var_in_dict <-    data.frame("var"=toupper(colNames_dataset)) %>% 
                          left_join(data_dictionary, by=c("var"="New.CCM.Item.Name")) %>% 
                          filter(!is.na(Description)) %>% 
                          select(var,Description)  
        
        var_position <- c()
        for (i in var_in_dict$var){
          var_position <- c(var_position,which(colNames_dataset==i))
        }
        var_in_dict$position <- var_position
    
        
        for (i in 1:dim(var_in_dict)[1]){
          names(dataset)[var_in_dict[i,3]] <- var_in_dict[i,2]
        }
        names(dataset) <- gsub("\\\\","",names(dataset))
        colnames(dataset)
```

## Produce some basic summaries
```{r basic summaries, echo=TRUE}
    #some numbers about frauds
        all_companies <- unique(dataset$gvkey)
        fraud_companies <- unique((dataset[dataset$understatement==1,]$gvkey))
        cat("total number of companies: ", length(all_companies))
        cat("\nnumber of fraud companies: ", length(fraud_companies))
        cat("\nfraud companies: ", fraud_companies)
        cat("\n\n")
        
        
        cat("\nProportion of non-fraud vs. fraud annual statements")
        prop.table(table(dataset$understatement)) #fraud reporting (understatement=1) /all reporting
        cat("\n\n")
        
        
        fraud_perc <- length(fraud_companies)/length(unique(dataset$gvkey)) #num. fraud companies/ all companies
        cat("Proportion of fraud companies: ", fraud_perc)
        cat("\n\n")
        
        cat("\nTotal number of annual statements: ", comma(nrow(dataset)))
        cat("\nTotal number of fraud statements: ", nrow(dataset[dataset$understatement==1,]))
        cat("\n")
        
        cat("\n\nContigency Table for statements labelled 'Issues'")
        round(prop.table(table(dataset$issue)),2) #what is this? what "issue" are we talking about?
    
    #check variables with missing values
        missing_value_count <- data.frame(missing_value_count= apply(dataset, 2, function(x){sum(is.na(x))})) %>% 
                                    filter(missing_value_count>=1)  
          
        missing_value_count$missing_var <- row.names(missing_value_count)
        row.names(missing_value_count) <- NULL
        missing_value_count <- missing_value_count[,c(2,1)]
        missing_value_df <-   missing_value_count %>% 
                              arrange(missing_value_count) %>% #asc on number of missing values
                              mutate(missing_value_perc= percent(missing_value_count/nrow(dataset)),
                                     missing_value_count =comma(missing_value_count))
        missing_value_df.new <- missing_value_df %>% 
                                left_join(data_dictionary, by=c("missing_var"= "Description")) %>% 
                                select(c(4,1,2,3)) %>% 
                                mutate(New.CCM.Item.Name=tolower(New.CCM.Item.Name)) %>% 
                                rename(missing_var_orig_coding=New.CCM.Item.Name)
        missing_value_df.new
        cat("\n\n")
    
    #check if we are missing different years' data
        companies_with_missing_year <- dataset %>% 
                                       select(gvkey,fyear) %>% 
                                       group_by(gvkey) %>%
                                       mutate(report_range = max(fyear)-min(fyear)+1,
                                              count_reports= n(),
                                              num_missing_year =report_range - count_reports,
                                              proportion_reports_missing =num_missing_year/report_range) %>% 
                                       distinct(gvkey,report_range,count_reports,num_missing_year,proportion_reports_missing) %>% 
                                       filter(num_missing_year>=1) %>% #get companies that have missed annual reports
                                       distinct(gvkey)
          
        num_company_with_missing_reports <- length(companies_with_missing_year$gvkey)
        cat("Number of companies with missing annual reports:", 
        comma(num_company_with_missing_reports)) #I wonder on which exchange most of these companies are listed
        cat("\nTotal number of companies:", comma(length(all_companies)))       
        cat("\nProportion of companies with missing annual reports:", percent(num_company_with_missing_reports/length(all_companies)))   
```


## Data Preparation
```{r train_test, echo=TRUE}
# convert the columns to appropriate types 
        dataset.new <- dataset
        dataset.new$p_aaer <- NULL  #"Accounting and Auditing Enforcement Releases" usually come after allegations
        dataset.new$new_p_aaer <- NULL  #"Accounting and Auditing Enforcement Releases" usually come after allegations
        c(num.of.rows, num.of.cols) %<-% dim(dataset.new) #get dimensions of the data frame
    
        
# for each company, get the earliest and latest year for which a record was available. 
        min_year <- min(dataset.new$fyear)
        max_year <- max(dataset.new$fyear)
        num_years <- max_year-min_year +1
        ref_years_df <- data.frame(gvkey= rep(unique(dataset.new$gvkey),each=num_years),
                                   fyear = rep(min_year:max_year, times=length(unique(dataset.new$gvkey))))
        
        dataset.new.mod <-ref_years_df %>% 
                              left_join(dataset.new,by=c("gvkey"="gvkey","fyear"= "fyear")) %>% 
                              group_by(gvkey) %>% 
                              arrange(gvkey,fyear)
        
        min_max_lookup <- dataset.new.mod %>% 
                              filter(!is.na(understatement)) %>% 
                              group_by(gvkey) %>% 
                              mutate(max_year=max(fyear),
                                     min_year=min(fyear)) %>% 
                              select(gvkey,max_year,min_year) %>% 
                              distinct(gvkey,max_year,min_year)
        
       dataset.new.mod<-  dataset.new.mod %>% 
                              left_join(min_max_lookup,by=c("gvkey"="gvkey")) %>% 
                              mutate(ind_missing = case_when(fyear <min_year~"pre", 
                                                             fyear >max_year~ "post",
                                                             fyear>min_year&fyear<max_year~"during",
                                                             TRUE~"N/A"))
       dataset.new.mod <- as.data.frame(dataset.new.mod)
              

#define "pre", "post" and "during" NA's 
       dataset.new.mod[dataset.new.mod$ind_missing=="pre",3:49] <- -100000000000 #before first file
       dataset.new.mod[dataset.new.mod$ind_missing=="post",3:49] <- 100000000000 #between first file and last file 
       dataset.new.mod[dataset.new.mod$ind_missing=="during",3:49] <- 100000000000 #after last file 
       
       cleaned_data <- dataset.new.mod
       cleaned_data$gvkey <- NULL 
       cleaned_data$max_year <- NULL      
       cleaned_data$min_year <- NULL 
       
       #str(dataset.new.mod)
       #summary(dataset.new.mod)
       
#fraud data (how are splits represented)                    
        fraud.df <- dataset.new.mod %>% filter(gvkey %in% fraud_companies)
        #View(fraud.df)
```
## Clustering 
```{r clustering, echo=TRUE, warning=FALSE}
filing_compliance <- dataset.new.mod %>% 
                            mutate(duration=max_year-min_year+1,
                                   has.filed= case_when(abs(understatement)<1~1,
                                                        TRUE~0)) %>% 
                            select(gvkey,has.filed,duration) %>% 
                            group_by(gvkey) %>%
                            mutate(filing_count=sum(has.filed),
                                   duration=max(duration)) %>% 
                            select(-has.filed)

filing_compliance <- filing_compliance %>% 
                          mutate(filing_rate =filing_count/duration ) %>% 
                          arrange(filing_rate) %>% 
                          distinct_all()

str(filing_compliance)

par(mfrow=c(2,1))
hist(filing_compliance[filing_compliance$gvkey %in% fraud_companies,]$filing_rate,
     main="filing rate distribution: fraud companies",xlab ="filing rate")
hist(filing_compliance[!filing_compliance$gvkey %in% fraud_companies,]$filing_rate,
     main="filing rate distribution: non-fraud companies",xlab ="filing rate")
```


## RNN 
```{r RNN, echo=TRUE}

    # library(keras)
    # library(tensorflow)
    # use_condaenv("r-reticulate", required = TRUE) #https://community.rstudio.com/t/error-installation-of-tensorflow-not-found-in-rstudio/67200/2
    # reticulate::py_config()
    # model <- keras_model_sequential() %>%
    #              layer_embedding(input_dim = max_features, output_dim = 32) %>%
    #              layer_simple_rnn(units = 32) %>%
    #              layer_dense(units = 1, activation = "sigmoid")
    # 
    # model %>% compile(
    #    optimizer = "rmsprop",
    #    loss = "binary_crossentropy",
    #    metrics = c("acc")
    # )
    # 
    # history <- model %>% fit(
    #    input_train, y_train,
    #    epochs = 10,
    #    batch_size = 128,
    #    validation_split = 0.2
    # )
    # 
    # plot(history)
        # library(keras)
    # library(tensorflow)
    # use_condaenv("r-reticulate", required = TRUE) #https://community.rstudio.com/t/error-installation-of-tensorflow-not-found-in-rstudio/67200/2
    # 
    # max_features <- 10000
    # maxlen <- 500
    # batch_size <- 32
    # cat("Loading data...\n")
    # imdb <- dataset_imdb(num_words = max_features)
    # c(c(input_train, y_train), c(input_test, y_test)) %<-% imdb
    # cat(length(input_train), "train sequences\n")
    # cat(length(input_test), "test sequences")
    # cat("Pad sequences (samples x time)\n")
    # input_train <- pad_sequences(input_train, maxlen = maxlen)
    # input_test <- pad_sequences(input_test, maxlen = maxlen)
    # cat("input_train shape:", dim(input_train), "\n")
    # cat("input_test shape:", dim(input_test), "\n")
```


## Autoencoder 
```{r autoencoder, echo=TRUE}
    
    # library(keras)
    # library(tensorflow)
    # use_condaenv("r-reticulate", required = TRUE) #https://community.rstudio.com/t/error-installation-of-tensorflow-not-found-in-rstudio/67200/2
    # 
    # max_features <- 10000
    # maxlen <- 500
    # batch_size <- 32
    # cat("Loading data...\n")
    # imdb <- dataset_imdb(num_words = max_features)
    # c(c(input_train, y_train), c(input_test, y_test)) %<-% imdb
    # cat(length(input_train), "train sequences\n")
    # cat(length(input_test), "test sequences")
    # cat("Pad sequences (samples x time)\n")
    # input_train <- pad_sequences(input_train, maxlen = maxlen)
    # input_test <- pad_sequences(input_test, maxlen = maxlen)
    # cat("input_train shape:", dim(input_train), "\n")
    # cat("input_test shape:", dim(input_test), "\n")
```